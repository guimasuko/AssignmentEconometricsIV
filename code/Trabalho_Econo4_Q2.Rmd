---
title: "Trabalho - Econometria IV"
author: "Guilherme Luz, Guilherme Masuko, Caio Garzeri"
date: "`r format(Sys.time(), '%B %Y')`"
output: 
  pdf_document: 
    keep_tex: true
geometry: margin=1in
#bibliography: references.bib 
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(cache = T, echo = T, tidy = TRUE, tidy.opts = list(width.cutoff = 60), warning = F, message = F)
```

```{r library, include=FALSE}
library(tidyverse)
library(ggplot2)
library(stargazer)
library(kableExtra)
library(gridExtra)
library(ggsci) # color pallette
library(tictoc)
library(readxl)
```

```{r}
library(lubridate) # for handling dates
library(zoo) # for time series
library(dynlm) # for time series regressions
library(forecast) # for the improved Pacf function
library(glmnet) # for shrinkage methods
library(HDeconometrics) # IC for glmnet

# Packages for parallel computation
library(future)
library(foreach)
library(doFuture)
library(doRNG)
```


```{r plot_setup, include=FALSE}
# Configurações 
theme_set(theme_linedraw())
theme_update(title = element_text(family = 'serif'),
             legend.text = element_text(family = 'serif')) 
```

# Question 2

First of all, we must do some data wrangling.

```{r}
# Import the data
raw_data = read_csv('data/2021-12.csv')
data0 = raw_data[-1,] %>% select_if(~ !any(is.na(.)))
transformation = raw_data[1,]
```

The suggested transformations (in order to make the series stationary) are indicated according to the following numeration.

Transformation codes (from FRED):

1. no transformation
2. $\Delta x_t$
3. $\Delta^2 x_t$ 
4. $\log(x_t)$
5. $\Delta \log(x_t)$
6. $\Delta^2 \log(x_t)$
7. $\Delta (x_t/x_{t-1} -1)$

For the CPI, we apply a specific transformation to turn it into an inflation series. \textcolor{blue}{Detalhe: estou usando a definição de inflação dada no enunciado $\pi_t = \frac{\Delta P_t}{P_t}$, que difere da definição usual $\pi_t = \frac{\Delta P_t}{P_{t-1}}$}

```{r}
# Data transformations based on the FRED transformation codes
data = data0 %>% select(-sasdate) %>% 
  rename(SP500="S&P 500", SPINDUST="S&P: indust") %>%
  BVAR::fred_transform(type = 'fred_md') %>%
  bind_cols(tibble(date = data0$sasdate[3:length(data0$sasdate)])) %>%
  mutate(date = as.Date(date, format = '%m/%d/%Y'))

# For the CPI, we transform into an inflation series
data = mutate(data, CPIAUCSL = 100*(diff(data0$CPIAUCSL, differences = 1)/data0$CPIAUCSL[-1])[-1])
```

```{r}
# Inflation as time series
inflation = data$CPIAUCSL %>% ts(start = c(year(data$date[1]),month(data$date[1])), frequency = 12)
```

The resulting inflation series, which we want to forecast is shown below.

```{r, }
# plot inflation

data %>% 
  select(date, CPIAUCSL) %>%
  mutate(date = as.Date(date, format = '%m/%d/%Y')) %>%
  ggplot(aes(x = date, y = CPIAUCSL))+
  geom_line()+
  labs(title = 'Inflation (CPI, % mom)', x = NULL, y = NULL)
```

Now, we shall start the estimations.

### AR

In order to get some idea of what the order of our AR(p) process is, we plot the partial autocorrelation of the inflation series for a particular window.

```{r}
# Partial autocorrelation
inflation %>%
  window(start = start(inflation), end = start(inflation) + c(0,492)) %>%
  Pacf( lag.max = 24, plot = T)
```

We believe that a maximum lag of 24 is more than reasonable. Then, the determine the actual order p based on the BIC.

```{r}
# Function for calculating the BIC for AR models
BIC.ar <- function(model) {
  
  ssr <- sum(model$resid^2, na.rm = T)
  t <- length(model$resid)
  npar <- length(model$coef)
  
  return(c("p" = model$order,
            "BIC" = log(ssr/t) + npar * log(t)/t))
}
```

We proceed with a rolling window one-step-ahead forecast, in which we choose the optimal order of the AR in each window of estimation.

```{r}
# Rolling window forecasting
rolling_window <- 492
p.max <- 24

forecast1 = list()

for(a in 0:(length(inflation)-rolling_window-1)){

  # get the window for training the model
train = window(inflation, 
               start = start(inflation)+c(0,a), 
               end = start(inflation)+c(0,a+rolling_window-1) )

bic.table = c()

for(p in 0:p.max){ # calculating the BIC for different orders of the AR(p)
  AR = ar(train, order.max = p, method = 'ols', aic = F)
  bic.line = BIC.ar(AR) 
  bic.table = rbind(bic.table, bic.line)
}
bic.table = data.frame(bic.table)

p.opt = bic.table$p[which.min(bic.table$BIC)] # pick the optimal p

AR =  ar(train, order.max = p.opt, method = 'ols', aic = F) # run the AR model with the optimal p

forecast1[[a+1]] = predict(AR, n.ahead = 1)$pred # one-step-ahead forecast
}

forecasts = forecast1 %>% unlist() %>% 
  ts(start = start(forecast1[[1]]), frequency =  frequency(forecast1[[1]]) )
```

```{r, echo = F}
inflation %>% window(start = c(1990,1)) %>%
ts.plot(forecasts, col = c("red", "blue"),
         gpars=list(ylab="Inflation", xlab=NULL, main = 'AR forecast'))
legend("bottomleft", inset =0.05, legend=c("Inflation", "Forecast"), 
       col = c("red", "blue"), lty=1)
```


### AR + PC

1. PCA

We do a Principal Component Analysis (PCA). Note that we must center and scale the data, since the series are in different scales. \textcolor{red}{depois pra fazer previsao deveria me preocupar com isso?}

```{r}
# PCA
pca = data %>% 
  select(-CPIAUCSL, -date) %>% 
  prcomp(center = TRUE, scale=TRUE)  
```

2. Select PCs

We can, then, choose the number of factors $k$ and select the first $k$ PCs. As seen in Question 1, there are different ways to choose the number of factors. We look at 3 common criterion (rule of thumb, informal way and biggest drop), but we opt for the rule of thumb as it seems to be the most parsimonious in this case. \textcolor{blue}{colocar alguma explicacao de qual criterio vamos adotar - usar rule of thumb q foi oq o masuko usou na 1}

```{r, echo=F}
pca.var <- pca$sdev^2
pca.var.prop = data.frame( pc = 1:length(pca.var), var.prop = pca.var/sum(pca.var)) %>%
  mutate(var.prop.cum = cumsum(var.prop))

pca.var.prop %>%
  ggplot(aes(x = pc, y = var.prop.cum)) +
  geom_bar(stat = 'identity', fill = '#00468BFF') +
  labs(x = "Principal Component", y = "Variance Explained") +
  ggtitle("Variance explained by principal components") 
```

```{r}
# Choosing the number of PCs

# Rule of thumb (3%)

pca.var.prop %>%
  filter(var.prop>=0.03) %>%
  nrow() %>%
  paste('(rule of thumb)')

# Informal way (90%)

pca.var.prop %>%
  filter(var.prop.cum<=0.9) %>%
  nrow() %>%
  paste('(informal way)')

# Biggest drop 

(lag(pca.var.prop$var.prop)/pca.var.prop$var.prop) %>%
  which.max() %>% -1 %>%
  paste('(biggest drop)')
```

```{r}
# Using the rule of thumb
n_pc = pca.var.prop %>%
  filter(var.prop>=0.03) %>%
  nrow()
```

3. Regression

Given the number of factors, the order of the autoregressive component is determined by BIC in each rolling window.

```{r}
# Get the factor from the PCA
Factors = pca$x[,1:n_pc]

# Create the data matrix with the factors
variables = cbind(inflation, Factors) 

variables_withdate = variables %>%
  bind_cols(date = as.Date.yearmon(time(inflation))) %>%
  setNames(c('inflation', colnames(Factors), 'date'))
```

We proceed with the rolling window one-step-ahead forecast.

```{r}
# Function for creating the proper data matrix based on the regression formula
  
  # Instead of manually creating data matrix, we use the dynlm() function and get only the $model component
create_datamatrix = function(train, p.opt){
  new = dynlm(inflation ~ L(inflation, 1:p.opt) + L(train[,-1] , 1), 
              data = ts(rbind(train,0), start = start(train), frequency = frequency(train)) )
  new = new$model %>% tail(1) %>% select(-inflation) %>% as.matrix()
  return(new)
}
```


```{r forecast_loop_AR_PC}
# Rolling window forecasting
rolling_window <- 492
p.max <- 24

forecast1 = list()
coefficients_pc1 <- list()

# set up parallel computation
registerDoFuture()
plan("multisession", workers = 3) # use 3 cores 

# Loop
forecast1 = foreach(a = 0:(length(inflation)-rolling_window-1)) %dorng% { 

train = window(variables, 
               start = start(inflation)+c(0,a), 
               end = start(inflation)+c(0,a+rolling_window-1) )

bic.table = rep(NA,p.max)

for(p in 1:p.max){ # calculating the BIC for different orders of the AR(p)
  AR_PC = dynlm(inflation ~ L(inflation, 1:p) + L(train[ ,-1] , 1), 
              data = train)
  bic.table[p] = BIC(AR_PC)
}

p.opt = which.min(bic.table) # pick the optimal p

AR_PC = dynlm(inflation ~ L(inflation, 1:p.opt) + L(train[,-1] , 1), 
              data = train)     # run the AR-PC model with the optimal p

new = create_datamatrix(train, p.opt)

result = AR_PC$coefficients %*% c(1, new) # one-step-ahead forecast
result
}

forecast1 = forecast1 %>% unlist() %>% 
  ts(start = start(inflation)+c(0,rolling_window), frequency =  frequency(inflation) )


# Loop to get coefficients
coefficients_pc1 = foreach(a = 0:(length(inflation)-rolling_window-1)) %dorng% { 

train = window(variables, 
               start = start(inflation)+c(0,a), 
               end = start(inflation)+c(0,a+rolling_window-1) )

bic.table = rep(NA,p.max)

for(p in 1:p.max){ # calculating the BIC for different orders of the AR(p)
  AR_PC = dynlm(inflation ~ L(inflation, 1:p) + L(train[ ,-1] , 1), 
              data = train)
  bic.table[p] = BIC(AR_PC)
}

p.opt = which.min(bic.table) # pick the optimal p

AR_PC = dynlm(inflation ~ L(inflation, 1:p.opt) + L(train[,-1] , 1), 
              data = train)     # run the AR-PC model with the optimal p

result = AR_PC[[1]] # one-step-ahead forecast
result
}

```

```{r, echo=F}
inflation %>% window(start = c(1990,1)) %>%
ts.plot(forecast1, col = c("red", "blue"),
         gpars=list(ylab="Inflation", xlab=NULL, main = 'AR+PC forecast'))
legend("bottomleft", inset =0.05, legend=c("Inflation", "Forecast"), 
       col = c("red", "blue"), lty=1)
```

```{r}
# Save forecasts
forecasts = cbind(AR = forecasts, AR_PC = forecast1) %>% as.ts()
```

### Ridge Regression

We will choose penalty term according to the BIC. However, we must decide on the number of lags in the model 
and this criterion is obviously silent about this issue. Our strategy will be to run the models with 1, 2, 3 and 4 lags and 
choose the model with the smallest MSE.

```{r}
# Embedding
# function that creates n_lags of all variables of a given data frame
my_embed = function(df, n_lags = 4){
Lags = list()
Lags[[1]] = df %>% select(-contains('date'))
if(n_lags>=1){
  for(i in 1:n_lags){
    Lags[[i+1]] = df %>% select(-contains('date')) %>% 
      mutate_all(function(x) lag(x, n = i))
  }
}
lagged_data = reduce(Lags, function(x,y){bind_cols(x,y, .name_repair = ~make.unique(.x))}) 

return(lagged_data)
}
```


```{r forecast_loop_Ridge}
tic()
# Rolling window forecasting
rolling_window <- 492

# glmnet parameter
my_alpha = 0 # Ridge

forecast1 = list()

# set up parallel computation
registerDoFuture()
plan("multisession", workers = 3) # use 3 cores 

forecast1 = foreach(a = 1:(length(inflation)-rolling_window)) %dorng% { 
  # get the window for training the model
  train = data[a:(a+rolling_window-1), ]
  # embed
  reg_data = my_embed(train)
  # bind the embeded columns with the one-step-ahead inflation
  reg_data = bind_cols(inflation.ahead = lead(inflation[a:(a+rolling_window-1)]), reg_data)
  
  # Ridge estimation
  ic_ridge <- ic.glmnet(x = reg_data %>% na.omit() %>% select(-inflation.ahead), 
                        y = reg_data %>% na.omit() %>% select(inflation.ahead) %>% data.matrix(), 
                        crit="bic", alpha= my_alpha)
  ridge <- glmnet(x = reg_data %>% na.omit() %>% select(-inflation.ahead), 
                  y = reg_data %>% na.omit() %>% select(inflation.ahead) %>% data.matrix(), 
                  alpha=my_alpha, lambda = ic_ridge$lambda)
  
  # Prediction
  new = reg_data %>% select(-inflation.ahead) %>% tail(1)
  result = predict(ridge, newx = data.matrix(new), s=ic_ridge$lambda)
  
  result
}

forecast1 = forecast1 %>% unlist() %>% 
  ts(start = start(inflation)+c(0,rolling_window), frequency =  frequency(inflation) )
toc()

beepr::beep()
```

```{r, echo=F}
inflation %>% window(start = c(1990,1)) %>%
ts.plot(forecast1, col = c("red", "blue"),
         gpars=list(ylab="Inflation", xlab=NULL, main = 'Ridge forecast',
                    sub = 'Using 4 lags of all variables'))
legend("bottomleft", inset =0.05, legend=c("Inflation", "Forecast"), 
       col = c("red", "blue"), lty=1)
```

```{r}
# Save forecasts
forecasts = cbind.zoo(forecasts, Ridge_4lags = forecast1) %>% as.ts()
```

The forecast of the Ridge regression with 4 lags has a notably bad fit to the actual inflation series. We noticed that, since the ridge is not able to give a sparse solution, when there are too many variables, the estimated model becomes basically an intercept and almost all the other coefficients are very close to zero (but not zero). Hence, we tested other (more parsimonious) specifications. When we include the all the macroeconomics variables - without any lags - and lags of the CPI, we get a more reasonable result. The results are very robust to the number of CPI lags, so we keep 4 lags, as initially intended. 

```{r forecast_loop_Ridge_other}
tic()
# Rolling window forecasting
rolling_window <- 492

# glmnet parameter
my_alpha = 0 # Ridge

forecast1 = list()

# set up parallel computation
registerDoFuture()
plan("multisession", workers = 3) # use 3 cores 

last_fcst = (length(inflation)-rolling_window)

forecast1 = foreach(a = 1:last_fcst) %dorng% { 
  # get the window for training the model
  train = data[a:(a+rolling_window-1), ] %>% select(-CPIAUCSL)
  train_cpi = data[a:(a+rolling_window-1), ] %>% select(CPIAUCSL)
  # embed
  reg_data = my_embed(train, n_lags = 0) 
  cpi_lags = my_embed(train_cpi, n_lags = 4)
  # bind the embeded columns with the one-step-ahead inflation
  reg_data = bind_cols(inflation.ahead = lead(inflation[a:(a+rolling_window-1)]), 
                       cpi_lags, reg_data)
  
  # Ridge estimation
  ic_ridge <- ic.glmnet(x = reg_data %>% na.omit() %>% select(-inflation.ahead), 
                        y = reg_data %>% na.omit() %>% select(inflation.ahead) %>% data.matrix(), 
                        crit="bic", alpha= my_alpha)
  ridge <- glmnet(x = reg_data %>% na.omit() %>% select(-inflation.ahead), 
                  y = reg_data %>% na.omit() %>% select(inflation.ahead) %>% data.matrix(), 
                  alpha=my_alpha, lambda = ic_ridge$lambda)
  
  # Prediction
  new = reg_data %>% select(-inflation.ahead) %>% tail(1)
  result = predict(ridge, newx = data.matrix(new), s=ic_ridge$lambda)
  
  result
}

forecast1 = forecast1 %>% unlist() %>% 
  ts(start = start(inflation)+c(0,rolling_window), frequency =  frequency(inflation) )
toc()

beepr::beep()
```

```{r, echo=F}
inflation %>% window(start = c(1990,1)) %>%
ts.plot(forecast1, col = c("red", "blue"),
         gpars=list(ylab="Inflation", xlab=NULL, main = 'Ridge forecast', 
                    sub = 'Using 4 lags of CPI and no lags of other variables'))
legend("bottomleft", inset =0.05, legend=c("Inflation", "Forecast"), 
       col = c("red", "blue"), lty=1)
```

```{r}
# Save forecasts
forecasts = cbind.zoo(forecasts, Ridge = forecast1) %>% as.ts()
```

### LASSO Regression

```{r forecast_loop_LASSO}
# Rolling window forecasting
rolling_window <- 492

# glmnet parameter
my_alpha = 1 # LASSO

forecast1 = list()
coefficients_lasso = list()


for(a in 1:(length(inflation)-rolling_window)){
  # get the window for training the model
  train = data[a:(a+rolling_window-1), ]
  # embed
  reg_data = my_embed(train)
  # bind the embeded columns with the one-step-ahead inflation
  reg_data = bind_cols(inflation.ahead = lead(inflation[a:(a+rolling_window-1)]), reg_data) 
  
  # Ridge estimation
  ic_lasso <- ic.glmnet(x = reg_data %>% na.omit() %>% select(-inflation.ahead), 
                        y = reg_data %>% na.omit() %>% select(inflation.ahead) %>% data.matrix(), 
                        crit="bic", alpha = my_alpha)
  lasso <- glmnet(x = reg_data %>% na.omit() %>% select(-inflation.ahead), 
                  y = reg_data %>% na.omit() %>% select(inflation.ahead) %>% data.matrix(),
                  alpha = my_alpha, lambda = ic_lasso$lambda)
  
  # Prediction
  new = reg_data %>% select(-inflation.ahead) %>% tail(1)
  forecast1[a] = predict(lasso, newx = data.matrix(new), s=ic_lasso$lambda)
  
  #Coefficients
  coefficients_lasso[a] = coef(lasso)
  
}

forecast1 = forecast1 %>% unlist() %>% 
  ts(start = start(inflation)+c(0,rolling_window), frequency =  frequency(inflation) )

```

```{r forecast_loop_LASSO_other, eval = F, include=FALSE}
# Alternative specifications of LASSO (with more inflation lags, etc)
# Not as good, so did not include in the final file

# Rolling window forecasting
rolling_window <- 492

# glmnet parameter
my_alpha = 1 # LASSO

forecast1 = list()

for(a in 1:(length(inflation)-rolling_window)){
  # get the window for training the model
  train = data[a:(a+rolling_window-1), ] %>% select(-CPIAUCSL)
  train_cpi = data[a:(a+rolling_window-1), ] %>% select(CPIAUCSL)
  # embed
  reg_data = my_embed(train, n_lags = 3) 
  cpi_lags = my_embed(train_cpi, n_lags = 12)
  # bind the embeded columns with the one-step-ahead inflation
  reg_data = bind_cols(inflation.ahead = lead(inflation[a:(a+rolling_window-1)]), 
                       cpi_lags, reg_data)
  
  # LASSO estimation
  ic_lasso <- ic.glmnet(x = reg_data %>% na.omit() %>% select(-inflation.ahead), 
                        y = reg_data %>% na.omit() %>% select(inflation.ahead) %>% data.matrix(), 
                        crit="bic", alpha = my_alpha)
  lasso <- glmnet(x = reg_data %>% na.omit() %>% select(-inflation.ahead), 
                  y = reg_data %>% na.omit() %>% select(inflation.ahead) %>% data.matrix(), 
                  alpha = my_alpha, lambda = ic_lasso$lambda)
  
  # Prediction
  new = reg_data %>% select(-inflation.ahead) %>% tail(1)
  forecast1[a] = predict(lasso, newx = data.matrix(new), s=ic_lasso$lambda)

}

forecast1 = forecast1 %>% unlist() %>% 
  ts(start = start(inflation)+c(0,rolling_window), frequency =  frequency(inflation) )
```

```{r, echo=F}
inflation %>% window(start = c(1990,1)) %>%
ts.plot(forecast1, col = c("red", "blue"),
         gpars=list(ylab="Inflation", xlab=NULL, main = 'LASSO forecast'))
legend("bottomleft", inset =0.05, legend=c("Inflation", "Forecast"), 
       col = c("red", "blue"), lty=1)
```

```{r}
# Save forecasts
forecasts = cbind.zoo(forecasts, LASSO = forecast1) %>% as.ts()
```

## Item A

```{r}
# Forecasting error
error = inflation - forecasts
cum_error = sapply(error, function(x){x^2 %>% cumsum()}) %>%
  bind_cols(date = as.Date.yearmon(time(error))) %>%
  setNames( c('AR', 'AR+PC', 'Ridge (4 lags)', 'Ridge', 'LASSO', 'date' ) )
```

```{r, echo = F}
# Plot 
cum_error %>% 
  gather(key = model, value = error, -date) %>% 
  ggplot(aes(x = date, y = error, color = model)) +
  geom_line(size=0.8) +
  geom_vline(xintercept = as.Date('2020-01-01'), linetype = 'dashed', size = 1)+
  scale_color_lancet()+
  labs(title = 'Cumulative squared errors', color = 'Model', y = NULL, x = NULL)
```




```{r}
# Save data for Question 3
save(data, inflation, forecasts, file = 'data/Q2_objects.Rda')
write.csv( forecasts, file = "output/forecasts.csv" )
write.csv( cum_error, file = "output/cum_error.csv" )
```


## Item B

We will follow the FRED-MD classification of variables into 8 groups: (i) output and income; (ii) labor market; (iii) housing; (iv) consumption,orders and inventories; (v) money and credit; (vi) interest and exchange rates; (vii) prices; and (viii) stock market. We are adding a ninth group called (ix) lags, with the lagged inflation series.


```{r}
#Get FRED groups
groups = read_xlsx('data/FRED-MD_updated_appendix.xlsx')
groups <- groups %>% select(fred, group) 

#Change some names manually because they have minor differences with the variable names in existing dataframe
groups$fred[groups$fred == "S&P 500"] <- "SP500"
groups$fred[groups$fred == "IPB51222s"] <- "IPB51222S"
groups$fred[groups$fred == "S&P: indust"] <- "SPINDUST"
	
names <- function(base_name, n) {
    new_name = paste0(base_name, ".", n)
    return(new_name)
}

# Expand group_df with new variable names
expandgroup <- groups %>%
  rowwise() %>%
  mutate(
    NewVariables = list(names(fred, 1:4)),
    NewGroups = list(rep(group, length(NewVariables)))
  ) %>%
  unnest(c(NewVariables, NewGroups)) %>%
  select(c(NewVariables, NewGroups)) %>%
  rename(fred = "NewVariables", group = "NewGroups")

# Merge with original group_df
endgroups <- bind_rows(groups, expandgroup)

# Sort by variable name
endgroups <- endgroups %>%
  arrange(fred)

#Change CPI lags to group "lags" (9)
endgroups$group[endgroups$fred == "CPIAUCSL"] <- 9
endgroups$group[endgroups$fred == "CPIAUCSL.1"] <- 9  
endgroups$group[endgroups$fred == "CPIAUCSL.2"] <- 9
endgroups$group[endgroups$fred == "CPIAUCSL.3"] <- 9
endgroups$group[endgroups$fred == "CPIAUCSL.4"] <- 9

groups<-endgroups

rm(endgroups, expandgroup, names)
```

```{r}
# Computing variable importance for LASSO

# Create a matrix to store coefficients
reg_data2 <- reg_data %>% select(-inflation.ahead) 
coeff_lasso <- data.frame(matrix(ncol = ncol(reg_data2), nrow = length(forecast1)))
colnames(coeff_lasso) <- colnames(reg_data2)

#Retrieve coefficients and variable identifiers from lists
var_lasso = modify_depth(coefficients_lasso, 1, "i")
co_lasso = modify_depth(coefficients_lasso, 1, "x")

for (i in 1:length(forecast1)){
  a = var_lasso[[i]] %>% unlist()
  b = co_lasso[[i]] %>% unlist()
    for (c in 2:length(a)){
      coeff_lasso[i,a[c]]<-b[c]
    }
}

rm(var_lasso, co_lasso)

#Multiply for sd
for (i in 1:length(forecast1)){
  coeff_lasso[,i] = coeff_lasso[,i]*sd(reg_data2[,i])
}

```

We compute the 10 most relevant predictors considering the mean absolute value of the coefficients over all estimation windows.

```{r}

coeff_lasso <- coeff_lasso %>% mutate_all(~ replace_na(., 0))

top10_lasso <-  colMeans(abs(coeff_lasso))

top_10_lasso <- coeff_lasso %>%
  summarise_all(~ mean(abs(.))) %>%
  pivot_longer(everything()) %>%
  arrange(desc(value)) %>%
  head(10)

top_10_lasso <- top_10_lasso %>% mutate(importance = 100* value/value[1]) %>% arrange(desc(importance))

ggplot(top_10_lasso, aes(x = reorder(name, -importance), y = importance)) +
  geom_bar(stat = "identity") +
  labs(title = "Variable Importance - LASSO",
       x = "Variables",
       y = "Importance") +
  theme(axis.text.x = element_text(angle = 45, hjust = 1))

```
We now present results for groups. Group 9 (previous inflation) is consisntetly the most important though less important throught the sample. Groups 6 (bond and exchange rates) and 7 (prices) are important.   Group (2) labor market used to be relevant. Now not so much.

```{r}
#Get sum over groups
# Sum over cells based on groups
coeff_long <- data.frame(variable = rep(colnames(coeff_lasso), each = nrow(coeff_lasso)),
                            row_index = rep(1:nrow(coeff_lasso), times = ncol(coeff_lasso)),
                            value = as.vector(as.matrix(coeff_lasso)))

coeff_long <- coeff_long %>% arrange(row_index)
groups <- groups %>% rename(variable="fred")
merged_data <- merge(transposed_df, groups, by = "variable", all.x = TRUE)
merged_data <- merged_data %>% arrange(row_index)

groupfinal_lasso <- merged_data %>% group_by(row_index, group) %>% summarise(total = sum(abs(value)))

wide_group_lasso <- groupfinal_lasso %>%
  pivot_wider(names_from = group, values_from = total) %>% ungroup()

wide_group_lasso_rel <- wide_group_lasso %>%
  mutate(across(-1, ~ . / rowSums(across(-1))))

wide_group_lasso_rel$dates <- as.Date(time(forecast1))

# Melt the dataframe to long format for plotting
melted_df <- melt(wide_group_lasso_rel, id.vars = "dates", variable.name = "Column")
melted_df <- melted_df %>% filter(Column != "row_index")
melted_df$Group <- as.integer(melted_df$Column) -1
melted_df$Group <- as.character(melted_df$Group)


# Create a stacked column plot
ggplot(data = melted_df, 
       aes(x = dates, y = value, fill = Group)) + 
  geom_area()

```

```{r}
# Computing variable importance for PC

# Get the alphas
alpha = as.matrix(pca$rotation[,1:n_pc])

#Get the lambdas (coefficients of the Factors) and the phis
lambdas = matrix(NA,261,6)
phis = matrix(NA,261,24)

for(i in 1:261 ){
  size = length(coefficients_pc1[[i]])
  lags = size - 1 - 6 # intercept and 6 factors
    for(j in 1:6){
      lambdas[i,j] <- coefficients_pc1[[i]][size-6+j]
      }
    for(l in 1:24){
      phis[i,l] <- coefficients_pc1[[i]][l+1]
      }
}
  
#Multiply alpha by lambdas to get "coefficient" of each variable in each window 
importpc = as.data.frame(alpha %*% t(lambdas))
phist = as.data.frame(t(phis))
row_names <- paste("CPIAUCSL", seq(1, 24), sep = ".")
importpc$fred = rownames(importpc)
phist$fred = row_names
importpc <- rbind(importpc, phist)

importpc = merge(importpc, groups, by="fred",  all.x = TRUE)
importpc$group <- ifelse(is.na(importpc$group), 9, importpc$group) # giving all lags of inflation group 9

```

Top 10 most relevant variables
```{r}
top_10_pc <- importpc %>%
  rowwise() %>%
  mutate(mean_abs = mean(abs(c_across(-c(fred, group))))) %>%
  ungroup() %>%
  select(fred, mean_abs) %>%
  arrange(desc(mean_abs)) %>%
  head(10)

top_10_pc <- top_10_pc %>% mutate(importance = 100* mean_abs/mean_abs[1]) %>% arrange(desc(importance))

ggplot(top_10_pc, aes(x = reorder(fred, -importance), y = importance)) +
  geom_bar(stat = "identity") +
  labs(title = "Variable Importance - PC",
       x = "Variables",
       y = "Importance") +
  theme(axis.text.x = element_text(angle = 45, hjust = 1))
```
Same shit 9, 7, 6, 2


```{r}
result <- importpc %>%
  mutate(across(starts_with("V"), ~ abs(.), .names = "abs_{.col}")) %>%
  group_by(group) %>%
  summarise(across(starts_with("abs_V"), ~ sum(., na.rm = TRUE)))

pc_rel <- result %>%
  mutate(across(starts_with("abs_V"), ~ . / sum(., na.rm = TRUE), .names = "rel_{.col}")) %>%
  select(starts_with("rel_"))
  
pc_rel_transposed <- as.data.frame((t(pc_rel)))

pc_rel_transposed <- pc_rel_transposed %>%
  mutate(date = as.Date(time(forecast1)))

importpc_long <- pc_rel_transposed %>%
  pivot_longer(cols = starts_with("V"), names_to = "variable", values_to = "value")

ggplot(importpc_long, aes(x = date, y = value, fill = variable)) +
  geom_area() +
  labs(titel = "Group Importance", x = "Date", y = "Value", fill = "Variable") 



```



